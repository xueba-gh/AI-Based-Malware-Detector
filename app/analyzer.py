
import pandas as pd
import numpy as np
from scapy.all import rdpcap, IP, TCP, UDP
import pickle
import os
from .utils import setup_logging
from collections import defaultdict
from datetime import datetime

logger = setup_logging()

def safe_division(numerator, denominator):
    return numerator / denominator if denominator != 0 else 0

def analyze_pcap(file):
    """
    Analyze the PCAP file and extract features.
    """
    try:
        # Read the PCAP file
        packets = rdpcap(file)

        # Initialize flow dictionary
        flows = defaultdict(lambda: {'fwd_packets': [], 'bwd_packets': []})

        # Extract features
        for packet in packets:
            if IP in packet:
                ip_layer = packet[IP]
                src_ip = ip_layer.src
                dst_ip = ip_layer.dst
                protocol = ip_layer.proto
                timestamp = float(packet.time)

                if TCP in packet:
                    transport_layer = packet[TCP]
                    src_port = transport_layer.sport
                    dst_port = transport_layer.dport
                    flags = int(transport_layer.flags)  # Convert FlagValue to int
                elif UDP in packet:
                    transport_layer = packet[UDP]
                    src_port = transport_layer.sport
                    dst_port = transport_layer.dport
                    flags = 0
                else:
                    continue

                flow_id = f"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{protocol}"
                reverse_flow_id = f"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{protocol}"

                packet_info = {
                    'timestamp': timestamp,
                    'length': len(packet),
                    'header_length': len(packet) - len(packet[IP].payload),
                    'flags': flags
                }

                if flow_id in flows:
                    flows[flow_id]['fwd_packets'].append(packet_info)
                elif reverse_flow_id in flows:
                    flows[reverse_flow_id]['bwd_packets'].append(packet_info)
                else:
                    flows[flow_id]['fwd_packets'].append(packet_info)

        # Compute flow features
        features = []
        for flow_id, flow_data in flows.items():
            src_ip, src_port, dst_ip, dst_port, protocol = parse_flow_id(flow_id)
            fwd_packets = flow_data['fwd_packets']
            bwd_packets = flow_data['bwd_packets']

            # Ensure we have at least one packet
            if not fwd_packets and not bwd_packets:
                continue

            all_packets = fwd_packets + bwd_packets
            flow_duration = max(p['timestamp'] for p in all_packets) - min(p['timestamp'] for p in all_packets)

            feature = {
                'Flow ID': flow_id,
                'Source IP': src_ip,
                'Source Port': int(src_port),
                'Destination IP': dst_ip,
                'Destination Port': int(dst_port),
                'Protocol': protocol,
                'Timestamp': min(p['timestamp'] for p in all_packets),
                'Flow Duration': flow_duration,
                'Total Fwd Packets': len(fwd_packets),
                'Total Backward Packets': len(bwd_packets),
                'Total Length of Fwd Packets': sum(p['length'] for p in fwd_packets),
                'Total Length of Bwd Packets': sum(p['length'] for p in bwd_packets),
                'Fwd Packet Length Max': max(p['length'] for p in fwd_packets) if fwd_packets else 0,
                'Fwd Packet Length Min': min(p['length'] for p in fwd_packets) if fwd_packets else 0,
                'Fwd Packet Length Mean': np.mean([p['length'] for p in fwd_packets]) if fwd_packets else 0,
                'Fwd Packet Length Std': np.std([p['length'] for p in fwd_packets]) if len(fwd_packets) > 1 else 0,
                'Bwd Packet Length Max': max(p['length'] for p in bwd_packets) if bwd_packets else 0,
                'Bwd Packet Length Min': min(p['length'] for p in bwd_packets) if bwd_packets else 0,
                'Bwd Packet Length Mean': np.mean([p['length'] for p in bwd_packets]) if bwd_packets else 0,
                'Bwd Packet Length Std': np.std([p['length'] for p in bwd_packets]) if len(bwd_packets) > 1 else 0,
                'Flow Bytes/s': safe_division(sum(p['length'] for p in all_packets), flow_duration),
                'Flow Packets/s': safe_division(len(all_packets), flow_duration),
                'Flow IAT Mean': np.mean(np.diff([p['timestamp'] for p in sorted(all_packets, key=lambda x: x['timestamp'])])) if len(all_packets) > 1 else 0,
                'Flow IAT Std': np.std(np.diff([p['timestamp'] for p in sorted(all_packets, key=lambda x: x['timestamp'])])) if len(all_packets) > 1 else 0,
                'Flow IAT Max': max(np.diff([p['timestamp'] for p in sorted(all_packets, key=lambda x: x['timestamp'])])) if len(all_packets) > 1 else 0,
                'Flow IAT Min': min(np.diff([p['timestamp'] for p in sorted(all_packets, key=lambda x: x['timestamp'])])) if len(all_packets) > 1 else 0,
                'Fwd IAT Total': sum(np.diff([p['timestamp'] for p in sorted(fwd_packets, key=lambda x: x['timestamp'])])) if len(fwd_packets) > 1 else 0,
                'Fwd IAT Mean': np.mean(np.diff([p['timestamp'] for p in sorted(fwd_packets, key=lambda x: x['timestamp'])])) if len(fwd_packets) > 1 else 0,
                'Fwd IAT Std': np.std(np.diff([p['timestamp'] for p in sorted(fwd_packets, key=lambda x: x['timestamp'])])) if len(fwd_packets) > 1 else 0,
                'Fwd IAT Max': max(np.diff([p['timestamp'] for p in sorted(fwd_packets, key=lambda x: x['timestamp'])])) if len(fwd_packets) > 1 else 0,
                'Fwd IAT Min': min(np.diff([p['timestamp'] for p in sorted(fwd_packets, key=lambda x: x['timestamp'])])) if len(fwd_packets) > 1 else 0,
                'Bwd IAT Total': sum(np.diff([p['timestamp'] for p in sorted(bwd_packets, key=lambda x: x['timestamp'])])) if len(bwd_packets) > 1 else 0,
                'Bwd IAT Mean': np.mean(np.diff([p['timestamp'] for p in sorted(bwd_packets, key=lambda x: x['timestamp'])])) if len(bwd_packets) > 1 else 0,
                'Bwd IAT Std': np.std(np.diff([p['timestamp'] for p in sorted(bwd_packets, key=lambda x: x['timestamp'])])) if len(bwd_packets) > 1 else 0,
                'Bwd IAT Max': max(np.diff([p['timestamp'] for p in sorted(bwd_packets, key=lambda x: x['timestamp'])])) if len(bwd_packets) > 1 else 0,
                'Bwd IAT Min': min(np.diff([p['timestamp'] for p in sorted(bwd_packets, key=lambda x: x['timestamp'])])) if len(bwd_packets) > 1 else 0,
                'Fwd PSH Flags': sum(p['flags'] & 0x08 for p in fwd_packets),
                'Bwd PSH Flags': sum(p['flags'] & 0x08 for p in bwd_packets),
                'Fwd URG Flags': sum(p['flags'] & 0x20 for p in fwd_packets),
                'Bwd URG Flags': sum(p['flags'] & 0x20 for p in bwd_packets),
                'Fwd Header Length': sum(p['header_length'] for p in fwd_packets),
                'Bwd Header Length': sum(p['header_length'] for p in bwd_packets),
                'Fwd Packets/s': safe_division(len(fwd_packets), flow_duration),
                'Bwd Packets/s': safe_division(len(bwd_packets), flow_duration),
                'Min Packet Length': min(p['length'] for p in all_packets),
                'Max Packet Length': max(p['length'] for p in all_packets),
                'Packet Length Mean': np.mean([p['length'] for p in all_packets]),
                'Packet Length Std': np.std([p['length'] for p in all_packets]) if len(all_packets) > 1 else 0,
                'Packet Length Variance': np.var([p['length'] for p in all_packets]) if len(all_packets) > 1 else 0,
                'FIN Flag Count': sum(p['flags'] & 0x01 for p in all_packets),
                'SYN Flag Count': sum(p['flags'] & 0x02 for p in all_packets),
                'RST Flag Count': sum(p['flags'] & 0x04 for p in all_packets),
                'PSH Flag Count': sum(p['flags'] & 0x08 for p in all_packets),
                'ACK Flag Count': sum(p['flags'] & 0x10 for p in all_packets),
                'URG Flag Count': sum(p['flags'] & 0x20 for p in all_packets),
                'CWE Flag Count': sum(p['flags'] & 0x40 for p in all_packets),
                'ECE Flag Count': sum(p['flags'] & 0x80 for p in all_packets),
                'Down/Up Ratio': safe_division(len(bwd_packets), len(fwd_packets)),
                'Average Packet Size': np.mean([p['length'] for p in all_packets]),
                'Avg Fwd Segment Size': np.mean([p['length'] for p in fwd_packets]) if fwd_packets else 0,
                'Avg Bwd Segment Size': np.mean([p['length'] for p in bwd_packets]) if bwd_packets else 0,
                'Fwd Header Length.1': sum(p['header_length'] for p in fwd_packets),
            }
            features.append(feature)

        # Convert to DataFrame
        df = pd.DataFrame(features)

        logger.info(f"Successfully analyzed PCAP file. Extracted {len(features)} flows.")
        return df

    except Exception as e:
        logger.error(f"Error analyzing PCAP file: {str(e)}", exc_info=True)
        raise


    #the rest remains unchange
def parse_flow_id(flow_id):
    src, dst, proto = flow_id.split('-')
    src_ip, src_port = src.split(':')
    dst_ip, dst_port = dst.split(':')
    return src_ip, src_port, dst_ip, dst_port, proto


def predict_malware(features):
    """
    Use the SVM model to predict if the traffic is malicious.
    """
    try:
        # Load the pre-trained model
        model_path = os.path.join('models', 'malcare.pkl')
        with open(model_path, 'rb') as f:
            model = pickle.load(f)

        # Ensure features are in the correct format
        X = features.drop(['Flow ID', 'Source IP', 'Destination IP', 'Timestamp'], axis=1, errors='ignore')  # Drop non-numeric columns
        X = X.fillna(0)  # Replace NaN values with 0

        # Make prediction
        prediction = model.predict(X)

        # Get prediction probability
        confidence = model.predict_proba(X).max()

        logger.info(f"Made prediction. Result: {'Malicious' if prediction[0] else 'Benign'}, Confidence: {confidence:.2f}")
        return prediction[0], confidence

    except Exception as e:
        logger.error(f"Error making prediction: {str(e)}", exc_info=True)
        raise